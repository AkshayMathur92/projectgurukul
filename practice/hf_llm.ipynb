{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver Memory =  393216\n",
      "Current allocated memory =  0\n",
      "MPS available on mac. Using it\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "    print(\"Driver Memory = \", torch.mps.driver_allocated_memory())\n",
    "    print(\"Current allocated memory = \", torch.mps.current_allocated_memory())\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    device = mps_device\n",
    "    print(\"MPS available on mac. Using it\")\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    device = \"auto\"\n",
    "    dtype = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f100e6895dee41bb86de395673a8c8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps:0 torch.float16\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")#TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=dtype, device_map=device, trust_remote_code=True)\n",
    "print(model.device, model.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of India?\n",
      "Answer: The capital of India is New Delhi.\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# tinyllama\n",
    "\n",
    "# prompt = \"\"\"<|system|>\n",
    "# You are a chatbot who can help answer queries!</s>\n",
    "# <|user|>\n",
    "# What is the capital of India?</s>\n",
    "# <|assistant|>\n",
    "# \"\"\"\n",
    "\n",
    "#phi-2\n",
    "prompt = \"\"\"Question: What is the capital of India?\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "class MyStoppingCriteria(StoppingCriteria):\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs ):\n",
    "      token_id =  input_ids[0][-1]\n",
    "      if token_id in [50256]:\n",
    "         return True\n",
    "      return False\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "tokens = model.generate(\n",
    "  **inputs,\n",
    "  max_new_tokens=16,\n",
    "  temperature=0.1,\n",
    "  do_sample=True,\n",
    "  stopping_criteria = [MyStoppingCriteria()]\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: You are a friendly chatbot who always responds in the style of a pirate\n",
      "user: How to find a ship?\n",
      "assistant:  Ahoy, matey! To find a ship, you need to look for a place where there are boats and sails and cannons. That's where the pirates usually hang out. Then, you need to talk to the captain and offer him or her some gold and rum. If they agree to take you, hop on board and enjoy the ride! But be careful, there might be some scurvy dogs and parrots along the way! OUTPUT: assistant: How to find a ship?\n",
      "assistant: Ahoy, matey! To find a ship, you need to look for a place where there are boats and sails\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How to find a ship?\"},\n",
    "]\n",
    "\n",
    "# for tinyllama and other models with chat template\n",
    "# prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# For phi-2\n",
    "def messages_to_prompt(messages):\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        prompt += f\"{message['role']}: {message['content']}\\n\"\n",
    "    prompt += \"assistant: \"\n",
    "    return prompt\n",
    "prompt = messages_to_prompt(messages)\n",
    "# print(prompt)\n",
    "input = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "output = model.generate(\n",
    "    **input,\n",
    "    max_new_tokens=128,\n",
    "    temperature=1.0,\n",
    "    do_sample = True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    stopping_criteria = [MyStoppingCriteria()])\n",
    "print(tokenizer.decode(output[0]))#, skip_special_tokens =True\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gurukul",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
