{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI, HuggingFaceLLM, ChatMessage\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from llama_index.prompts import PromptTemplate\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../README.md') as f:\n",
    "    context =  f.read()\n",
    "\n",
    "CONTEXT_PROMPT = '''\n",
    "Use the context to answer the question.\n",
    "Do not use prior knowledge outside the context.\n",
    "----------\n",
    "\n",
    "Context : {context_str}\n",
    "\n",
    "----------\n",
    "Question : {question_str}\n",
    "'''\n",
    "\n",
    "question = \"In 2 lines describe how to set up project gurukul? \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI\n",
    "openaillm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"<|system|> Your job is to answer users question.\n",
    "\"\"\"\n",
    "\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = PromptTemplate(\"<|user|>{query_str}<|assistant|>\")\n",
    "\n",
    "    \n",
    "huggingllm = HuggingFaceLLM(\n",
    "    context_window=2048,\n",
    "    is_chat_model=True,\n",
    "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    # model=model,\n",
    "    tokenizer_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    max_new_tokens = 64, \n",
    "    stopping_ids= [2],\n",
    "    generate_kwargs={\"temperature\": 0.01, 'do_sample':True},\n",
    "    model_kwargs={\"torch_dtype\": torch.float16},\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    )\n",
    "\n",
    "def messages_to_prompt(messages):\n",
    "    messages_dict = [\n",
    "                {\"role\": message.role.value, \"content\": message.content}\n",
    "                for message in messages\n",
    "            ]\n",
    "    return huggingllm._tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "huggingllm.messages_to_prompt = messages_to_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='mps', index=0), torch.float16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingllm._model.device, huggingllm._model.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = huggingllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a step-by-step guide to setting up Project Gurukul:\n",
      "\n",
      "1. Sign up for a free account on the Project Gurukul website.\n",
      "\n",
      "2. Click on the \"Create a Project\" button on the homepage.\n",
      "\n",
      "3. F\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = llm.complete(question)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To set up project Gurukul, follow these steps:\n",
      "\n",
      "1. Create a new virtual environment using the command `conda create -n gurukul python=3.11`.\n",
      "2. Activate the virtual environment using `conda activate gurukul`.\n",
      "3. Install\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(CONTEXT_PROMPT.format(context_str = context, question_str = question))\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: .env is a file that contains environment variables that are used by your Python application. Here's what you should write in it:\n",
      "\n",
      "```\n",
      "# .env file\n",
      "\n",
      "# Set the environment variable for the database URL\n",
      "DATABASE_URL=postgresql://username:password@localhost:5432\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages =     [\n",
    "        ChatMessage(role = 'system', content = 'Talk like a python and ML engineer expert. Give short answers.'),\n",
    "        ChatMessage(role = 'user', content = 'What should be written in .env?')\n",
    "    ]\n",
    "response = llm.chat(\n",
    "    messages\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: In the `.env` file, you should write the following:\n",
      "\n",
      "```\n",
      "OPENAI_API_KEY=<Your OpenAI Key>\n",
      "```\n",
      "\n",
      "Replace `<Your OpenAI Key>` with your actual OpenAI API key. This key is used to authenticate your requests to the OpenAI API\n"
     ]
    }
   ],
   "source": [
    "CHAT_SYSTEM_PROMPT = '''Talk like a python and ML engineer expert. Give short answers.\n",
    "                    Use the context to answer the question.\n",
    "Do not use prior knowledge outside the context.\n",
    "----------\n",
    "\n",
    "Context : {context_str}\n",
    "\n",
    "----------\n",
    "'''\n",
    "\n",
    "messages =     [\n",
    "        ChatMessage(role = 'system', content = CHAT_SYSTEM_PROMPT.format(context_str = context)),\n",
    "        ChatMessage(role = 'user', content = 'What should be written in .env?')\n",
    "    ]\n",
    "response = llm.chat(\n",
    "    messages\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gurukul",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
